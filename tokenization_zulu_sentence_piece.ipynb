{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frsysm-QMXH1",
        "outputId": "c67256a8-6a9c-4141-9f66-fd54c5dea033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: link drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Read the text file and get vocab size of unique words from a text file with english sentences\n",
        "\n",
        "with open(\"/content/drive/MyDrive/dataset-en.txt\", \"r\") as f:\n",
        "  text = f.read()\n",
        "  words = text.split()\n",
        "  unique_words = set(words)\n",
        "  vocab_size = len(unique_words)\n",
        "  print(\"Vocabulary size:\", vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzOFqRF9NTU4",
        "outputId": "83e69d5a-3926-448e-f3be-fd04ae1813bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 26677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8PAmE3KgOPy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Read the text file and get vocab size of unique words from a text file with english sentences\n",
        "\n",
        "with open(\"/content/drive/MyDrive/dataset-zu.txt\", \"r\") as f:\n",
        "  text = f.read()\n",
        "  words = text.split()\n",
        "  unique_words = set(words)\n",
        "  vocab_size = len(unique_words)\n",
        "  print(\"Vocabulary size:\", vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d5b4d6e-fbd0-4c24-a2d2-09fe1de0ac3b",
        "id": "Mvbp7BL6OQCf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 45339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "JS2qERPYMb5V"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.train(input='/content/drive/MyDrive/dataset-en.txt', model_prefix='/content/drive/MyDrive/smp_en', vocab_size=10000)\n"
      ],
      "metadata": {
        "id": "3UYbOpvqMeJ7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.train(input='/content/drive/MyDrive/dataset-zu.txt', model_prefix='/content/drive/MyDrive/smp_zu', vocab_size=20000)\n"
      ],
      "metadata": {
        "id": "hX5P3GywPx29"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Load SentencePiece model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "spz = spm.SentencePieceProcessor()\n",
        "sp.load('/content/drive/MyDrive/smp_en.model')\n",
        "spz.load('/content/drive/MyDrive/smp_zu.model')\n",
        "\n",
        "# Display some vocabulary items\n",
        "print(\"Vocabulary Sample english:\")\n",
        "for id in range(10):  # Just show first 10 entries\n",
        "    print(id, sp.id_to_piece(id))\n",
        "\n",
        "print(\"Vocabulary Sample zulu:\")\n",
        "for id in range(10):  # Just show first 10 entries\n",
        "    print(id, spz.id_to_piece(id))\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"No leadership in removing the anti-Semites.\"\n",
        "sentence_zu = \"Abukho ubuholi obungasusa abaphikisana nama-Semites.\"\n",
        "# Tokenize the sentence\n",
        "tokens = sp.encode_as_pieces(sentence)\n",
        "print(\"Tokens english:\", tokens)\n",
        "\n",
        "tokens_zu = spz.encode_as_pieces(sentence_zu)\n",
        "print(\"Tokens zulu:\", tokens_zu)\n",
        "\n",
        "# Get token IDs\n",
        "token_ids = sp.encode_as_ids(sentence)\n",
        "print(\"Token IDs English:\", token_ids)\n",
        "\n",
        "token_ids_zu = spz.encode_as_ids(sentence_zu)\n",
        "print(\"Token IDs Zulu:\", token_ids_zu)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgY36i6sOdY_",
        "outputId": "29a6358e-03d8-44fd-dc8b-d51e06965859"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Sample english:\n",
            "0 <unk>\n",
            "1 <s>\n",
            "2 </s>\n",
            "3 ▁the\n",
            "4 s\n",
            "5 ,\n",
            "6 ▁to\n",
            "7 ▁a\n",
            "8 .\n",
            "9 ▁and\n",
            "Vocabulary Sample zulu:\n",
            "0 <unk>\n",
            "1 <s>\n",
            "2 </s>\n",
            "3 -\n",
            "4 ▁\n",
            "5 ,\n",
            "6 ▁\"\n",
            "7 .\n",
            "8 a\n",
            "9 .\"\n",
            "Tokens english: ['▁No', '▁leadership', '▁in', '▁remov', 'ing', '▁the', '▁anti', '-', 'Semit', 'es', '.']\n",
            "Tokens zulu: ['▁', 'Abukho', '▁u', 'buholi', '▁obunga', 'susa', '▁aba', 'phikisana', '▁nama', '-', 'Semite', 's', '.']\n",
            "Token IDs English: [652, 1966, 13, 4247, 15, 3, 818, 18, 6524, 97, 8]\n",
            "Token IDs Zulu: [4, 7724, 10, 2648, 3533, 906, 43, 768, 176, 3, 10901, 25, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Load SentencePiece model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "spz = spm.SentencePieceProcessor()\n",
        "sp.load('/content/drive/MyDrive/smp_en.model')\n",
        "spz.load('/content/drive/MyDrive/smp_zu.model')\n",
        "\n",
        "# Display some vocabulary items\n",
        "print(\"Vocabulary Sample english:\")\n",
        "for id in range(10):  # Just show first 10 entries\n",
        "    print(id, sp.id_to_piece(id))\n",
        "\n",
        "print(\"Vocabulary Sample zulu:\")\n",
        "for id in range(10):  # Just show first 10 entries\n",
        "    print(id, spz.id_to_piece(id))\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"Doctors are susceptible to other diseases, such as alcoholism and addiction\"\n",
        "sentence_zu = \"Abukho ubuholi obungasusa abaphikisana nama-Semites.\"\n",
        "# Tokenize the sentence\n",
        "tokens = sp.encode_as_pieces(sentence)\n",
        "print(\"Tokens english:\", tokens)\n",
        "\n",
        "tokens_zu = spz.encode_as_pieces(sentence_zu)\n",
        "print(\"Tokens zulu:\", tokens_zu)\n",
        "\n",
        "# Get token IDs\n",
        "token_ids = sp.encode_as_ids(sentence)\n",
        "print(\"Token IDs English:\", token_ids)\n",
        "\n",
        "token_ids_zu = spz.encode_as_ids(sentence_zu)\n",
        "print(\"Token IDs Zulu:\", token_ids_zu)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euyAGuJjQgLG",
        "outputId": "3b47b09e-8a49-4d7e-9856-ac52128587c6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Sample english:\n",
            "0 <unk>\n",
            "1 <s>\n",
            "2 </s>\n",
            "3 ▁the\n",
            "4 s\n",
            "5 ,\n",
            "6 ▁to\n",
            "7 ▁a\n",
            "8 .\n",
            "9 ▁and\n",
            "Vocabulary Sample zulu:\n",
            "0 <unk>\n",
            "1 <s>\n",
            "2 </s>\n",
            "3 -\n",
            "4 ▁\n",
            "5 ,\n",
            "6 ▁\"\n",
            "7 .\n",
            "8 a\n",
            "9 .\"\n",
            "Tokens english: ['▁Do', 'ctor', 's', '▁are', '▁su', 'scepti', 'ble', '▁to', '▁other', '▁diseases', ',', '▁s', 'uch', '▁as', '▁a', 'lcoholism', '▁and', '▁addiction']\n",
            "Tokens zulu: ['▁', 'Abukho', '▁u', 'buholi', '▁obunga', 'susa', '▁aba', 'phikisana', '▁nama', '-', 'Semite', 's', '.']\n",
            "Token IDs English: [735, 4055, 4, 39, 1108, 6481, 1085, 6, 110, 3512, 5, 165, 247, 36, 7, 9307, 9, 4966]\n",
            "Token IDs Zulu: [4, 7724, 10, 2648, 3533, 906, 43, 768, 176, 3, 10901, 25, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lcu6z1pca--q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}